---
title: "Recovery survey exploration"
format: html
editor: visual
---

## Initial exploration of recovery survey data

Explore recovery surveys to examine emergent trends, survey effort, data structure, etc.

## Part 1: explore quadrat data

Step 1: set directories and load data. Note: data are stored on a secured server that requires authentication.

```{r}
rm(list=ls())
require(librarian)
librarian::shelf(tidyverse, ggplot2, RColorBrewer, vegan)

datdir <- "/Volumes/seaotterdb$/kelp_recovery/data/MBA_kelp_forest_database"

quad_dat <- read_csv(file.path(datdir,"processed/recovery/recovery_quad.csv"))

urch_size_dat <- read_csv(file.path(datdir,"processed/recovery/recovery_urch_sizefq.csv")) 

kelp_dat <- read_csv(file.path(datdir,"processed/recovery/recovery_kelpswath.csv")) 

gonad_dat <- read_csv(file.path(datdir, "processed/dissection_data_recovery.csv")) 

```

Step 2: prep derived datasets

```{r}

#Step 1: aggregate quad data to transect level
quad_transect <- quad_dat %>%
                  #drop substrate for now, we'll handle this later
                  select(-substrate, -quadrat)%>%
                  group_by(site, site_type, survey_date, latitude, longitude,
                           zone, transect) %>%
                  dplyr::summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))

#check nrow
nrow(quad_transect)
nrow(kelp_dat)
#wooo they match!

#Step 2: aggregate dissection data to transect level
gonad_zone <- gonad_dat %>%
                  #drop substrate for now, we'll handle this later
                  select(site, site_type, zone, species, test_diameter_mm,
                         gonad_index)%>%
                  group_by(site, site_type, zone, species) %>%
                  dplyr::summarise(across(everything(), ~ mean(.x, na.rm = TRUE))) %>%
                pivot_wider(
                  names_from = species,
                  values_from = c(test_diameter_mm, gonad_index)
                )

#step 3: join data 
merge_dat <- quad_transect %>%
              left_join(., gonad_zone, by = c("site", "site_type", "zone")) %>%
              left_join(., kelp_dat, by = c("survey_date","site","site_type", "zone","transect"
                                            ,"latitude", "longitude"
                                            )) %>%
  #reorder
  select(site, site_type, survey_date, latitude, longitude, zone, transect,depth_m, relief_cm,
          risk_cm, risk_index, purple_urchin_densitym2, purple_urchin_densitym2, red_urchin_densitym2, red_urchin_conceiledm2, tegula_densitym2, pomaulax_densitym2, 
          test_diameter_mm_purple_urchin,test_diameter_mm_red_urchin, gonad_index_purple_urchin, gonad_index_red_urchin, n_macro_plants_20m2,    macro_stipe_density_20m2, macro_stipe_sd_20m2, density20m2_purps_on_kelp, density20m2_ptecal, density20m2_eisarb, density20m2_nerlue, density20m2_lamset,              density20m2_cancer_spp, density20m2_lamstump,density20m2_macstump, everything())


```

Explore sampling effort for each site

```{r}
#| echo: false


# Reorder zone to put Shallow before Deep
quad_dat <- quad_dat %>%
  mutate(zone = factor(zone, levels = c("Shallow", "Deep")))

# Count unique quadrats per site, site type, zone, and transect
quadrat_counts <- quad_dat %>%
  group_by(site, site_type, zone, transect) %>%
  summarise(num_quadrats = n_distinct(quadrat), .groups = "drop")

# Bar plot with Dark2 color palette
ggplot(quadrat_counts, aes(x = num_quadrats, y = as.factor(transect), fill = site_type)) +
  geom_col(position = "dodge") +
  facet_grid(zone ~ site, scales = "free_y") +  # Facet by zone and site
  scale_fill_manual(values = brewer.pal(3, "Dark2")) +  # Use Dark2 colors
  labs(
    x = "Number of sampled quadrats",
    y = "Transect",
    title = "Number of quadrats per site, site type, zone, and transect",
    fill = "Site Type"
  ) +
  theme_minimal(base_size = 10) +
  theme(
    strip.text = element_text(face = "bold"),  # Bold facet labels
    panel.grid.major.y = element_blank(),      # Clean up y-axis grid
    panel.grid.minor = element_blank(),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10, angle = 0, hjust = 0.5)
  )


```

Check for missing or zero values

```{r}
#| echo: false


# Check missing values
missing_summary <- quad_dat %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "num_missing")

# Check zeros in density and percent cover columns
zero_summary <- quad_dat %>%
  summarise(across(where(is.numeric), ~sum(. == 0))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "num_zeros")

print(zero_summary)

```

Plot sea urchin density by zone and substrate type

```{r}
ggplot(quad_dat, aes(x = substrate, y = purple_urchin_densitym2, fill = site_type)) +
  geom_boxplot() +
  facet_wrap(~ zone) +
  labs(
    x = "Substrate type",
    y = "Purple sea urchin density (m²)",
    title = "Purple sea urchin density by substrate and zone"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
# Pivot data for easier boxplot creation
density_data <- quad_dat %>%
  select(site, zone, starts_with("purple_urchin_densitym2"), starts_with("red_urchin_densitym2")) %>%
  pivot_longer(cols = starts_with("purple_urchin_densitym2"):starts_with("red_urchin_densitym2"), 
               names_to = "density_type", values_to = "density")

# Boxplot to check for outliers
ggplot(density_data, aes(x = density_type, y = density)) +
  geom_boxplot(outlier.colour = "red", fill = "lightgray") +
  facet_wrap(~ zone) +
  labs(
    x = "Density Type",
    y = "Density (m²)",
    title = "Boxplot of Urchin Densities by Zone"
  ) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Part 2: explore site type differences

First, lets take a look at any biological (in terms of the cover and density of macroalgae and inverts) differences between forests, barrens, and incipient forests. We will normalize the data to handle both density and cover.

```{r}
# ------------------------------
# Step 0: Load Required Libraries
# ------------------------------
library(vegan)
library(ggplot2)

# ------------------------------
# Step 1: Define the Community Variables
# ------------------------------
# Include any columns starting with "cov_" plus the additional biological measures.
community_vars <- c(
  grep("^cov_", colnames(merge_dat), value = TRUE),
  "red_urchin_densitym2",
  "red_urchin_conceiledm2",
  "tegula_densitym2",
  "pomaulax_densitym2",
  "test_diameter_mm_purple_urchin",
  "test_diameter_mm_red_urchin",
  "gonad_index_purple_urchin",
  "gonad_index_red_urchin",
  "n_macro_plants_20m2",
  "macro_stipe_density_20m2",
  "density20m2_purps_on_kelp",
  "density20m2_ptecal",
  "density20m2_eisarb",
  "density20m2_nerlue",
  "density20m2_lamset",
  "density20m2_cancer_spp",
  "density20m2_lamstump",
  "density20m2_macstump",
  "purple_urchin_conceiledm2"
)

# Subset the data to include only the community variables
community_data <- merge_dat[, community_vars]

# ------------------------------
# Step 2: Remove Missing Data and Constant Variables
# ------------------------------
# Remove rows with missing values in community_data
complete_idx <- complete.cases(community_data)
community_data_complete <- community_data[complete_idx, ]
meta_complete <- merge_dat[complete_idx, ]  # Contains site_type, etc.

# Remove constant columns (those with zero variance) to avoid issues during scaling
non_constant <- sapply(community_data_complete, function(x) sd(x, na.rm = TRUE)) > 0
community_data_complete <- community_data_complete[, non_constant]

# Ensure site_type is a factor
meta_complete$site_type <- as.factor(meta_complete$site_type)

# ------------------------------
# Step 3: Standardize the Data
# ------------------------------
# Standardize the data (z-scores: mean=0, sd=1)
community_data_std <- scale(community_data_complete)

# ------------------------------
# Step 4: Run NMDS on the Standardized Community Data
# ------------------------------
set.seed(123)  # For reproducibility
nmds_comm <- metaMDS(community_data_std, distance = "euclidean", trymax = 100)
cat("NMDS stress:", nmds_comm$stress, "\n")

# Extract NMDS site scores (each transect is one point)
nmds_scores <- as.data.frame(scores(nmds_comm, display = "sites"))
nmds_scores$site_type <- meta_complete$site_type

# ------------------------------
# Step 5: Plot NMDS with Group Ellipses Using ggplot2
# ------------------------------
p_nmds <- ggplot(nmds_scores, aes(x = NMDS1, y = NMDS2, color = site_type)) +
  geom_point(size = 3) +
  stat_ellipse(type = "norm", level = 0.68, linetype = 2) +  # ~1 SD ellipse
  labs(title = "NMDS of Biological Community Data",
       x = "NMDS1", y = "NMDS2",
       color = "Site Type") +
  theme_minimal()
print(p_nmds)

# ------------------------------
# Step 6: Global PERMANOVA Analysis
# ------------------------------
# Compute Euclidean distance matrix on the standardized data
comm_dist <- dist(community_data_std, method = "euclidean")

# Run global PERMANOVA to test overall differences among site types
global_perm <- adonis2(comm_dist ~ meta_complete$site_type, permutations = 999)
print(global_perm)

# ------------------------------
# Step 7: Pairwise PERMANOVA with Corrected Indexing
# ------------------------------
pairwise_adonis2 <- function(dist_matrix, groups, perm = 999, p.adjust.method = "bonferroni") {
  groups <- as.factor(groups)
  pair_list <- combn(levels(groups), 2, simplify = FALSE)
  results <- data.frame()
  
  # Convert the 'dist' object to a full matrix for subsetting
  dmat <- as.matrix(dist_matrix)
  
  for (pair in pair_list) {
    # Select indices of observations in the current pair
    idx <- groups %in% pair
    # Subset the full distance matrix and convert back to a "dist" object
    dpair <- as.dist(dmat[idx, idx])
    ad_res <- adonis2(dpair ~ groups[idx], permutations = perm)
    res <- data.frame(Group1 = pair[1],
                      Group2 = pair[2],
                      F.Model = ad_res$F[1],
                      R2 = ad_res$R2[1],
                      p.value = ad_res$`Pr(>F)`[1])
    results <- rbind(results, res)
  }
  results$p.adjusted <- p.adjust(results$p.value, method = p.adjust.method)
  return(results)
}

pairwise_results <- pairwise_adonis2(comm_dist, meta_complete$site_type, perm = 999)
print(pairwise_results)

# ------------------------------
# Step 8: SIMPER Analysis to Determine What Drives Differences
# ------------------------------
simper_results <- simper(community_data_std, group = meta_complete$site_type, permutations = 999)
simper_summary <- summary(simper_results)
print(simper_summary)



```

Now lets separate density and cover, sticking with biological data only, for now.

```{r}
# ------------------------------
# Step 0: Load Required Libraries
# ------------------------------
library(vegan)
library(ggplot2)
library(dplyr)

# ------------------------------
# Step 1: Define Helper Function for Pairwise PERMANOVA
# ------------------------------
pairwise_adonis2 <- function(dist_matrix, groups, perm = 999, p.adjust.method = "bonferroni") {
  groups <- as.factor(groups)
  pair_list <- combn(levels(groups), 2, simplify = FALSE)
  results <- data.frame()
  # Convert the "dist" object to a full matrix for subsetting
  dmat <- as.matrix(dist_matrix)
  for (pair in pair_list) {
    idx <- groups %in% pair
    dpair <- as.dist(dmat[idx, idx])
    ad_res <- adonis2(dpair ~ groups[idx], permutations = perm)
    res <- data.frame(Group1 = pair[1],
                      Group2 = pair[2],
                      F.Model = ad_res$F[1],
                      R2 = ad_res$R2[1],
                      p.value = ad_res$`Pr(>F)`[1])
    results <- rbind(results, res)
  }
  results$p.adjusted <- p.adjust(results$p.value, method = p.adjust.method)
  return(results)
}

# ====================================================
# Analysis 1: Cover Data (Columns beginning with "cov_")
# ====================================================
cat("### Cover Data Analysis ###\n")

# 1. Subset cover data:
cover_data <- merge_dat[, grep("^cov_", colnames(merge_dat))]

# 2. Remove rows with missing values and extract metadata:
complete_idx_cover <- complete.cases(cover_data)
cover_data_complete <- cover_data[complete_idx_cover, ]
meta_cover <- merge_dat[complete_idx_cover, ]
meta_cover$site_type <- as.factor(meta_cover$site_type)

# 3. Remove constant columns (zero variance):
non_constant_cover <- sapply(cover_data_complete, function(x) sd(x, na.rm = TRUE)) > 0
cover_data_complete <- cover_data_complete[, non_constant_cover]

# 4. (NO scaling is applied – raw data are used directly)

# 5. Run NMDS on raw cover data (using Euclidean distance):
set.seed(123)
nmds_cover <- metaMDS(cover_data_complete, distance = "euclidean", trymax = 100)
cat("NMDS stress (Cover):", nmds_cover$stress, "\n")

# 6. Extract NMDS site scores and add group information:
scores_cover <- as.data.frame(scores(nmds_cover, display = "sites"))
scores_cover$site_type <- meta_cover$site_type

# 7. Plot NMDS with ggplot2 and draw ellipses:
p_cover <- ggplot(scores_cover, aes(x = NMDS1, y = NMDS2, color = site_type)) +
  geom_point(size = 3) +
  stat_ellipse(type = "norm", level = 0.68, linetype = 2) +
  labs(title = "NMDS of Cover Data (Raw Values)",
       x = "NMDS1", y = "NMDS2",
       color = "Site Type") +
  theme_minimal()
print(p_cover)

# 8. Global PERMANOVA on Cover Data:
dist_cover <- dist(cover_data_complete, method = "euclidean")
perm_cover <- adonis2(dist_cover ~ meta_cover$site_type, permutations = 999)
print(perm_cover)

# 9. Pairwise PERMANOVA on Cover Data:
pairwise_cover <- pairwise_adonis2(dist_cover, meta_cover$site_type, perm = 999)
print(pairwise_cover)

# 10. SIMPER Analysis for Cover Data:
simper_cover <- simper(cover_data_complete, group = meta_cover$site_type, permutations = 999)
simper_summary_cover <- summary(simper_cover)
print(simper_summary_cover)

# ====================================================
# Analysis 2: Density Data (Selected density and size variables)
# ====================================================
cat("\n### Density Data Analysis ###\n")

# 1. Define the density variables:
density_vars <- c(
  "red_urchin_densitym2",
  "red_urchin_conceiledm2",
  "tegula_densitym2",
  "pomaulax_densitym2",
  "test_diameter_mm_purple_urchin",
  "test_diameter_mm_red_urchin",
  "gonad_index_purple_urchin",
  "gonad_index_red_urchin",
  "n_macro_plants_20m2",
  "macro_stipe_density_20m2",
  "density20m2_purps_on_kelp",
  "density20m2_ptecal",
  "density20m2_eisarb",
  "density20m2_nerlue",
  "density20m2_lamset",
  "density20m2_cancer_spp",
  "density20m2_lamstump",
  "density20m2_macstump",
  "purple_urchin_conceiledm2"
)

# 2. Subset the density data:
density_data <- merge_dat[, density_vars]

# 3. Remove rows with missing values and extract metadata:
complete_idx_density <- complete.cases(density_data)
density_data_complete <- density_data[complete_idx_density, ]
meta_density <- merge_dat[complete_idx_density, ]
meta_density$site_type <- as.factor(meta_density$site_type)

# 4. Remove constant columns (zero variance):
non_constant_density <- sapply(density_data_complete, function(x) sd(x, na.rm = TRUE)) > 0
density_data_complete <- density_data_complete[, non_constant_density]

# 5. (NO scaling is applied – raw data are used directly)

# 6. Run NMDS on raw density data:
set.seed(123)
nmds_density <- metaMDS(density_data_complete, distance = "euclidean", trymax = 100)
cat("NMDS stress (Density):", nmds_density$stress, "\n")

# 7. Extract NMDS scores and add group information:
scores_density <- as.data.frame(scores(nmds_density, display = "sites"))
scores_density$site_type <- meta_density$site_type

# 8. Plot NMDS with ggplot2 and draw ellipses:
p_density <- ggplot(scores_density, aes(x = NMDS1, y = NMDS2, color = site_type)) +
  geom_point(size = 3) +
  stat_ellipse(type = "norm", level = 0.68, linetype = 2) +
  labs(title = "NMDS of Density Data (Raw Values)",
       x = "NMDS1", y = "NMDS2",
       color = "Site Type") +
  theme_minimal()
print(p_density)

# 9. Global PERMANOVA on Density Data:
dist_density <- dist(density_data_complete, method = "euclidean")
perm_density <- adonis2(dist_density ~ meta_density$site_type, permutations = 999)
print(perm_density)

# 10. Pairwise PERMANOVA on Density Data:
pairwise_density <- pairwise_adonis2(dist_density, meta_density$site_type, perm = 999)
print(pairwise_density)

# 11. SIMPER Analysis for Density Data:
simper_density <- simper(density_data_complete, group = meta_density$site_type, permutations = 999)
simper_summary_density <- summary(simper_density)
print(simper_summary_density)













# ------------------------------
# Example: Overlay Physical Factors Using envfit on the Cover NMDS
# ------------------------------

# Assuming the cover analysis has been completed with the following objects:
# - nmds_cover: The NMDS result (using raw cover data)
# - meta_cover: The metadata for cover analysis (includes 'site_type' and physical variables)

# 1. Define the physical variables you want to overlay.
#    Make sure these variables exist in meta_cover.
physical_vars <- meta_cover[, c("relief_cm", "risk_index")]  
# (Adjust the variable names as needed.)

# 2. Fit the physical variables onto the NMDS ordination using envfit.
envfit_phys <- envfit(nmds_cover, physical_vars, permutations = 999)
print(envfit_phys)  # Review the output to see significance and R² values.

# 3. Extract the envfit vectors (scores) for plotting.
envfit_scores <- as.data.frame(scores(envfit_phys, display = "vectors"))
envfit_scores$variable <- rownames(envfit_scores)

# 4. Overlay the envfit vectors onto the ggplot NMDS (from the cover analysis).
#    We assume you already have an NMDS plot stored in p_cover from previous code.
library(grid)  # For arrow unit specifications
p_cover + 
  geom_segment(data = envfit_scores,
               aes(x = 0, y = 0, xend = NMDS1, yend = NMDS2),
               arrow = arrow(length = unit(0.2, "cm")), color = "blue", size = 1) +
  geom_text(data = envfit_scores,
            aes(x = NMDS1, y = NMDS2, label = variable),
            color = "blue", hjust = 0.5, vjust = -0.5)



```

Toy with LDA

```{r}

# Load required libraries
library(MASS)
library(ggplot2)

# Ensure site_type is a factor
merge_dat$site_type <- as.factor(merge_dat$site_type)

# Option 1: Using a pre-selected subset of predictors that you believe are important.
# Here we choose key predictors related to physical characteristics and biological densities.
# You may modify these predictors based on your understanding of the ecology.
lda_model <- lda(site_type ~ relief_cm + risk_index + 
                   purple_urchin_densitym2 + red_urchin_densitym2 + 
                   tegula_densitym2 + cov_crustose_coralline + cov_encrusting_red,
                 data = merge_dat)

# Print LDA model details to see discriminant function coefficients and explained proportions
print(lda_model)

# Obtain the predicted linear discriminant values for each observation
lda_pred <- predict(lda_model)
# Create a data frame of LDA scores and add site_type for plotting
lda_df <- data.frame(lda_pred$x, site_type = merge_dat$site_type)

# Plot the first two linear discriminants
ggplot(lda_df, aes(x = LD1, y = LD2, color = site_type)) +
  geom_point(size = 3) +
  labs(title = "LDA: Separation of Site Types",
       x = "Linear Discriminant 1",
       y = "Linear Discriminant 2")







# Extract the scaling (coefficients) matrix from the LDA model
lda_coeff <- lda_model$scaling
print(lda_coeff)  # Examine the coefficients

# Optionally, visualize the absolute coefficients
library(reshape2)
library(ggplot2)

# Convert the matrix to a data frame for easier plotting
ld_loadings <- as.data.frame(lda_coeff)
ld_loadings$Variable <- rownames(ld_loadings)

# Reshape the data frame for ggplot2 (melt the data)
ld_loadings_melt <- melt(ld_loadings, id.vars = "Variable", 
                         variable.name = "Discriminant", value.name = "Coefficient")

# Create a bar plot of the absolute value of coefficients for each discriminant
ggplot(ld_loadings_melt, aes(x = reorder(Variable, abs(Coefficient)), y = Coefficient, fill = Discriminant)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "LDA: Predictor Contributions by Discriminant",
       x = "Predictor",
       y = "Coefficient Value")



```

Toy with permanova

```{r}
# Load necessary library
library(vegan)

# Select only numeric variables (and filter out constant variables)
numeric_vars <- merge_dat[, sapply(merge_dat, is.numeric)]
non_constant_vars <- numeric_vars[, sapply(numeric_vars, function(x) sd(x, na.rm = TRUE)) > 0]

# Scale the data (optional, but often useful)
scaled_vars <- scale(non_constant_vars)

# Compute a distance matrix (Euclidean distance is used here)
diss_matrix <- dist(scaled_vars)

# Run the global PERMANOVA
global_perm <- adonis2(diss_matrix ~ site_type, data = merge_dat, permutations = 999)
print(global_perm)


dispersion <- betadisper(diss_matrix, merge_dat$site_type)
anova(dispersion)  # Tests for differences in dispersion among groups










# Load required library
library(vegan)

# ------------------------
# 1. Remove Missing Values
# ------------------------
# Remove rows with missing values from the scaled data.
complete_ind <- complete.cases(scaled_vars)
scaled_vars_complete <- scaled_vars[complete_ind, ]
merge_dat_complete <- merge_dat[complete_ind, ]

# ------------------------
# 2. Define the Pairwise PERMANOVA Function Using adonis2
# ------------------------
pairwise.adonis2 <- function(x, factors, sim.method = 'euclidean', 
                             p.adjust.m = 'bonferroni', permutations = 999, ...) {
  # Ensure the vegan package is loaded
  library(vegan)
  
  # Get unique pairs of levels from the factors
  factor_levels <- unique(factors)
  combinations <- combn(factor_levels, 2)
  
  # Prepare a storage data.frame
  results <- data.frame()
  
  for (i in 1:ncol(combinations)) {
    # Create a logical index for rows belonging to the two groups in this pair
    current_pair <- factors %in% combinations[, i]
    # Perform PERMANOVA with adonis2
    ad_result <- adonis2(x[current_pair, ] ~ factors[current_pair],
                         permutations = permutations, 
                         method = sim.method, ...)
    
    # Extract F, R2 and p-value from the first row of the results table
    temp <- data.frame(
      group1 = combinations[1, i],
      group2 = combinations[2, i],
      F.Model = ad_result$F[1],
      R2 = ad_result$R2[1],
      p.value = ad_result$`Pr(>F)`[1]
    )
    
    results <- rbind(results, temp)
  }
  
  # Adjust p-values for multiple comparisons
  results$p.adjusted <- p.adjust(results$p.value, method = p.adjust.m)
  return(results)
}

# ------------------------
# 3. Run Pairwise PERMANOVA Comparisons
# ------------------------
pairwise_results <- pairwise.adonis2(scaled_vars_complete, 
                                     merge_dat_complete$site_type, 
                                     sim.method = "euclidean", 
                                     p.adjust.m = "bonferroni")

# Display the pairwise PERMANOVA results
print(pairwise_results)



```

TEst SIMPER

```{r}
# Load vegan if not already loaded
library(vegan)

# Run SIMPER on the complete scaled variables, grouped by site_type.
simper_results <- simper(scaled_vars_complete, merge_dat_complete$site_type)

# View a summary for a specific comparison, e.g., BAR vs. INCIP:
summary(simper_results, ordered = TRUE, group = "FOR-INCIP")


```

Test NMDS on cover

```{r}
# Load required libraries
library(vegan)
library(ggplot2)
library(grid)      # For arrow unit specifications

### 1. Subset and prepare the cover data ###
# Extract only the cover variables (names beginning with "cov_")
cover_vars <- merge_dat[, grep("^cov_", colnames(merge_dat))]

# Remove rows with missing values in the cover data
complete_idx <- complete.cases(cover_vars)
cover_vars_complete <- cover_vars[complete_idx, ]
merge_dat_cover <- merge_dat[complete_idx, ]

### 2. Run NMDS on the cover data ###
set.seed(123)  # For reproducibility
# Use Bray-Curtis distance which is common for percent cover data.
nmds_cover <- metaMDS(cover_vars_complete, distance = "bray", trymax = 100, autotransform = FALSE)

# Display the stress value to assess goodness-of-fit
print(nmds_cover$stress)

### 3. Extract NMDS scores into a data frame ###
# 'scores' extracts the site (observation) scores; we assume 2 dimensions.
nmds_scores <- as.data.frame(scores(nmds_cover, display = "sites"))
# Add the group (site_type) information
nmds_scores$site_type <- merge_dat_cover$site_type

### 4. Fit environmental variables using envfit ###
# Choose a set of environmental predictors you suspect might be driving differences.
env_vars <- merge_dat_cover[, c("relief_cm", "risk_index", 
                                "purple_urchin_densitym2", 
                                "red_urchin_densitym2", 
                                "tegula_densitym2")]

envfit_result <- envfit(nmds_cover, env_vars, permutations = 999)
print(envfit_result)

# Extract only the vectors (for numeric variables) that are significant (p < 0.05)
sig_idx <- which(envfit_result$vectors$pvals < 0.05)
if(length(sig_idx) > 0){
  sig_vectors <- envfit_result$vectors$arrows[sig_idx, , drop = FALSE]
  # Convert to a data frame and add variable names
  vectors_df <- as.data.frame(sig_vectors)
  vectors_df$Variable <- rownames(vectors_df)
  
  # Scale the vectors for plotting.
  # Choose a multiplier that makes the arrows visually appropriate relative to your NMDS scores.
  multiplier <- 0.2
  vectors_df$NMDS1 <- vectors_df$NMDS1 * multiplier
  vectors_df$NMDS2 <- vectors_df$NMDS2 * multiplier
} else {
  vectors_df <- NULL
}

### 5. Plot the NMDS using ggplot2 with ellipses and environmental vectors ###
# Create the base scatterplot of NMDS scores colored by site_type
p <- ggplot(nmds_scores, aes(x = NMDS1, y = NMDS2, color = site_type)) +
  geom_point(size = 3) +
  # Add ellipses around each group; stat_ellipse with level = 0.68 approximates 1 standard deviation
  stat_ellipse(type = "norm", level = 0.68, linetype = 2) +
  labs(title = "NMDS of Cover Data with Group Ellipses and Envfit Vectors",
       x = "NMDS1", y = "NMDS2",
       color = "Site Type") +
  theme_minimal()

# Add the envfit vectors if any are significant
if(!is.null(vectors_df)){
  p <- p +
    geom_segment(data = vectors_df,
                 aes(x = 0, y = 0, xend = NMDS1, yend = NMDS2),
                 arrow = arrow(length = unit(0.25, "cm")),
                 color = "black", size = 1) +
    geom_text(data = vectors_df,
              aes(x = NMDS1, y = NMDS2, label = Variable),
              color = "black", vjust = -0.5)
}

# Print the final plot
print(p)


```
